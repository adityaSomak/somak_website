+++
abstract = ""
abstract_short = ""
authors = ["Somak Aditya", "Yezhou Yang", "Chitta Baral", "Cornelia Fermuller", "Yiannis Aloimonos"]
date = "2013-07-01"
image_preview = ""
math = true
publication_types = ["1"]
publication = "In ArXiv Preprint. 2015"
publication_short = "In *arXiv preprint arXiv:1511.03292*"
selected = true
title = "From Images to Sentences through Scene Description Graphs using Commonsense Reasoning and Knowledge"
url_pdf = "http://arxiv.org/pdf/1511.03292v1.pdf"
url_project = "https://imagesdg.wordpress.com/image-to-scene-description-graph/"
# url_slides = "#"


# Optional featured image (relative to `static/img/` folder).
[header]
image = "headers/sdg.png"
caption = "My caption :smile:"

+++

In this paper we propose the construction of linguistic
descriptions of images. This is achieved through the extraction
of scene description graphs (SDGs) from visual scenes
using an automatically constructed knowledge base. SDGs
are constructed using both vision and reasoning. Specifically,
commonsense reasoning1
is applied on (a) detections
obtained from existing perception methods on given
images, (b) a “commonsense” knowledge base constructed
using natural language processing of image annotations
and (c) lexical ontological knowledge from resources such
as WordNet. Amazon Mechanical Turk(AMT)-based evaluations
on Flickr8k, Flickr30k and MS-COCO datasets show
that in most cases, sentences auto-constructed from SDGs
obtained by our method give a more relevant and thorough
description of an image than a recent state-of-the-art image
caption based approach. Our Image-Sentence Alignment
Evaluation results are also comparable to that of the recent
state-of-the art approaches.
